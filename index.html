<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation</title>
	<meta name="author" content="CaffNet">

	<link href="./bootstrap.min.css" rel="stylesheet">
    <link href="./style.css" rel="stylesheet">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
</head>

  <body>

    <div class="container">
      <div class="header">
        <div style="font-size: 10px">
        <h1> <center> Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation </center> </h1>
        <h4 style="color: #517CB9; font-size: 30px"> 
		<center> 
			<a href="http://cvpr2021.thecvf.com/"> <img src="./cvpr2021logo.jpg" style="max-width:25%"></a> 
		</center>
	</h4>
        </div>
      </div>

      <h3><center>
      <iframe width="100%" height="512" src="https://www.youtube.com/embed/9R2qQ7dGTp8" allowfullscreen></iframe>
      </center></h3>

      <div class="row">
      	<h3>Authors</h3>
      	<div style="font-size: 16px">
      	<ul>
		<li><a href="https://easy00.github.io">Jiyoung Lee</a><span style="text-transform:uppercase">&#10034;</span></li>
		<li><a href="https://soowhanchung.github.io/">Soo-Whan Chung</a><span style="text-transform:uppercase">&#10034;</span></li>
		<li>Sunok Kim</li>
		<li><a href="http://dsp.yonsei.ac.kr/">Hong-Goo Kang</a><span style="text-transform:uppercase">&#9841;</span></li>
		<li><a href="https://diml.yonsei.ac.kr/">Kwanghoon Sohn</a><span style="text-transform:uppercase">&#9841;</span></li>
      	</ul>
      	</div>
      	<p style="text-align: justify;"><span style="text-transform:uppercase">&#10034;</span> equal contribution</p>    
      	<p style="text-align: justify;"><span style="text-transform:uppercase">&#9841;</span> co-corresponding authors</p>    

      </div>

      <div class="row">
        <h3>Abstract</h3>
	<img src="./fig1_main.png" style="max-width:100%;padding-right:20px;" align="center">
        <p style="text-align: justify;">
        The objective of this paper is to separate a target speaker's speech from a mixture of two speakers using a deep audio-visual speech separation network. Unlike previous works that used lip movement on video clips or pre-enrolled speaker information as an auxiliary conditional feature, we use a single face image of the target speaker. In this task, the conditional feature is obtained from facial appearance in cross-modal biometric task, where audio and visual identity representations are shared in latent space. Learnt identities from facial images enforce the network to isolate matched speakers and extract the voices from mixed speech. It solves the permutation problem caused by swapped channel outputs, frequently occurred in speech separation tasks. The proposed method is far more practical than video-based speech separation since user profile images are readily available on many platforms. Also, unlike speaker-aware separation methods, it is applicable on separation with unseen speakers who have never been enrolled before. We show strong qualitative and quantitative results on challenging real-world examples. 
        </p>  
      </div>
      <div class="row">
      	<h3>Framework</h3>
        <p style="text-align: justify;">
        <img src="./fig_network.PNG" style="max-width:100%;padding-right:20px;" align="left">
	
        <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
	Overall network configuration: (1) encoding individual audio and visual features; (2) learning cross-modal affinity; (3) predicting spectrogram soft mask $\mathbf{M}$ to reconstruct target speech $\mathbf{\hat{Y}}$. Red dotted region means the magnitude operation processing.
        </p>
	<img src="./fig_spectrogram.PNG" style="max-width:100%;padding-right:20px;" align="center">
      </div>

      <div class="row">
        <h3>Paper</h3>
	<p>
     </p><table>
  <tbody><tr></tr>
  <tr><td>
    <a href="/"><img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="./thb.png" width="150px"></a>
  </td>
  <td></td>
  <td>
    J. Lee*, S.-W. Chung*, S. Kim, H.-G. Kang, K. Sohn<br>
    <b>Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation</b> <br>
    [<a href=/">Paper</a>] [<a href="/">Code</a>]
</td></tr></tbody></table>
     
      <h3>BibTeX</h3>
     <pre><tt>@inproceedings{lee2021looking,
  	title={Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation},
  	author={Lee, Jiyoung and Chung, Soo-Whan and Kim, Sunok and Kang, Hong-Goo and Sohn, Kwanghoon},
  	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  	year={2021}
	}</tt></pre>
      
      <div class="row">
        <h3>Acknowledgements</h3>
        <p>
        This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT). (NRF-2021R1A2C2006703). <br>
	This research was supported by the Yonsei University Research Fund(Yonsei Signature Research Cluster Program).
        
	</p>
      </div>

    </div>
