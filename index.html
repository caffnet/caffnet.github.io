<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation</title>
	<meta name="author" content="SLV-lab">

	<link href="./bootstrap.min.css" rel="stylesheet">
    <link href="./style.css" rel="stylesheet">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
</head>

  <body>

    <div class="container">
      <div class="header">
        <div style="font-size: 10px">
        <h1> <center> Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation </center> </h1>
        <h4 style="color: #517CB9; font-size: 30px"> <center> <a href="http://cvpr2021.thecvf.com/"><b>CVPR-2021</b></a> </center></center></h4>
        </div>
      </div>

      <h3><center>
      <iframe width="100%" height="512" src="https://www.youtube.com/embed/ku9xoLh62E4" allowfullscreen></iframe>
      </center></h3>

      <div class="row">
      	<h3>Authors</h3>
      	<div style="font-size: 16px">
      	<ul>
		<li><a href="https://easy00.github.io">Jiyoung Lee</a><span style="text-transform:uppercase">&#10034;</span></li>
		<li><a href="https://soowhanchung.github.io/">Soo-Whan Chung</a><span style="text-transform:uppercase">&#10034;</span></li>
		<li>Sunok Kim</li>
		<li><a href="https://dsp.yonsei.ac.kr/">Hong-Goo Kang</a><span style="text-transform:uppercase">&#9841;</span></li>
		<li><a href="https://diml.yonsei.ac.kr/">Kwanghoon Sohn</a><span style="text-transform:uppercase">&#9841;</span></li>
      	</ul>
      	</div>
      	<p style="text-align: justify;"><span style="text-transform:uppercase">&#10034;</span> equal contribution</p>    
      	<p style="text-align: justify;"><span style="text-transform:uppercase">&#9841;</span> co-corresponding authors</p>    

      </div>

      <div class="row">
        <h3>Abstract</h3>
	<img src="./fig1_main.png" style="max-width:100%;padding-right:20px;" align="center">
        <p style="text-align: justify;">
        <img src="./MNAD_files/teaser.png" style="max-width:55%;float:right;padding-left:20px;">The objective of this paper is to separate a target speaker's speech from a mixture of two speakers using a deep audio-visual speech separation network. Unlike previous works that used lip movement on video clips or pre-enrolled speaker information as an auxiliary conditional feature, we use a single face image of the target speaker. In this task, the conditional feature is obtained from facial appearance in cross-modal biometric task, where audio and visual identity representations are shared in latent space. Learnt identities from facial images enforce the network to isolate matched speakers and extract the voices from mixed speech. It solves the permutation problem caused by swapped channel outputs, frequently occurred in speech separation tasks. The proposed method is far more practical than video-based speech separation since user profile images are readily available on many platforms. Also, unlike speaker-aware separation methods, it is applicable on separation with unseen speakers who have never been enrolled before. We show strong qualitative and quantitative results on challenging real-world examples. 
        </p>  
      </div>
      <div class="row">
      	<h3>Framework</h3>
        <p style="text-align: justify;">
        <img src="./fig1_main.png" style="max-width:65%;padding-right:20px;" align="left">
        <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
        Overview of our framework for reconstructing a video frame. Our model mainly consists of three parts: an encoder, a memory module, and a decoder. The encoder extracts a query map $\mathbf{q}_t$ of size $H\times W \times C$ from an input video frame ${\bf{I}}_t$ at time $t$. The memory module performs reading and updating items $\mathbf{p}_m$ of size $1\times 1\times C$ using queries $\mathbf{q}_t^k$ of size $1\times 1\times C$, where the numbers of items and queries are $M$ and $K$, respectively, and $K=H\times W$. The query map $\mathbf{q}_t$ is concatenated with the aggregated items $\hat {\bf{p}}_t$. The decoder then inputs them to reconstruct the video frame $\hat {\bf{I}}_t$. For the prediction task, we input four successive video frames to predict the fifth one.
        </p>
        
      </div>

      <div class="row">
        <h3>Paper</h3>
	<p>
     </p><table>
  <tbody><tr></tr>
  <tr><td>
    <a href="/"><img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="./thb.png" width="150px"></a>
  </td>
  <td></td>
  <td>
    J. Lee*, S. Chung*, S. Kim, H. Kang, K. Sohn<br>
    <b>Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation</b> <br>
    [<a href=/">Paper</a>] [<a href="/">Code</a>]
</td></tr></tbody></table>
     
      <h3>BibTeX</h3>
     <pre><tt>@inproceedings{lee2020looking,
  	title={Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation},
  	author={Lee, Jiyoung and Chung, Soo-Whan and Kim, Sunok and Kang, Hong-Goo and Sohn, Kwanghoon},
  	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  	year={2021}
	}</tt></pre>
      
      <div class="row">
        <h3>Acknowledgements</h3>
        <p>
        This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT). (NRF-2021R1A2C2006703).
	This research was supported by the Yonsei University Research Fund(Yonsei Signature Research Cluster Program).
        
	</p>
      </div>

    </div>
